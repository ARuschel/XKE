{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "from tools import dataset_tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config, models\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from Äœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main variables\n",
    "dataset_name               = \"NELL186\"\n",
    "embedding_model            = models.TransE\n",
    "model_timestamp            = '1524632595'\n",
    "knn_k                      = 5 # number of nearest neighbors\n",
    "g_hat_fname_ids            = 'positives2id_{}nn.tsv'.format(knn_k)\n",
    "g_hat_fname_names          = 'positives_{}nn.tsv'.format(knn_k)\n",
    "neg_rate                   = 5 # negative to positive ratio\n",
    "bern                       = True\n",
    "feature_extractors         = ['pra', 'onesided', 'anyrel'] # pra, onesided or anyrel\n",
    "\n",
    "# GPU settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # should be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './benchmarks/{}/'.format(dataset_name)\n",
    "import_path = './results/{}/{}/{}/'.format(\n",
    "    dataset_name,\n",
    "    embedding_model.__name__,\n",
    "    model_timestamp\n",
    ")\n",
    "pra_explain_path = import_path + '/pra_explain/'\n",
    "pra_explain_path_abs = os.path.abspath(import_path + '/pra_explain/')\n",
    "experiment_specs_path = pra_explain_path + '/experiment_specs/'\n",
    "distribution = 'bern' if bern else 'unif'\n",
    "split_name = 'g_hat_{}nn_{}negrate_{}'.format(knn_k, neg_rate, distribution)\n",
    "g_hat_path_ids = os.path.abspath(import_path + '/g_hat/' + g_hat_fname_ids)\n",
    "g_hat_path_names = os.path.abspath(import_path + '/g_hat/' + g_hat_fname_names)\n",
    "\n",
    "# ensure dirs exist\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "ensure_dir(pra_explain_path)\n",
    "ensure_dir(experiment_specs_path)\n",
    "\n",
    "# handle feature extraction strings and split name\n",
    "feature_extractor_dict = {\n",
    "    'pra': 'PraFeatureExtractor',\n",
    "    'onesided': 'OneSidedFeatureExtractor',\n",
    "    'anyrel': 'AnyRelFeatureExtractor'\n",
    "}\n",
    "spec_g_hat_name = split_name + '__'\n",
    "feat_list = []\n",
    "for feat in feature_extractors:\n",
    "    spec_g_hat_name += '_' + feat\n",
    "    feat_list.append('\"{}\"'.format(feature_extractor_dict[feat]))\n",
    "feat_extractor_string = ','.join(feat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate/Read Negative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_filename = 'train2id_{}_{}to1.txt'.format(distribution, neg_rate)\n",
    "corrupted_dirpath = dataset_path + '/corrupted/'\n",
    "corrupted_filepath = corrupted_dirpath + corrupted_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(corrupted_filepath):\n",
    "    # create corrupted dirpath if not exist\n",
    "    if not os.path.exists(corrupted_dirpath):\n",
    "        os.makedirs(corrupted_dirpath)\n",
    "    # generate corrupted set and save to disk in `corrupted` folder\n",
    "    corrupted = dataset_tools.generate_corrupted_training_examples(dataset_path,\n",
    "            neg_proportion=neg_rate, bern=bern)\n",
    "    train2id = pd.DataFrame(corrupted)\n",
    "    train2id.to_csv(corrupted_filepath,\n",
    "        columns=['head', 'tail', 'relation', 'label'], index=False, header=False, sep=' ')\n",
    "    print('Created corrupted file: {}.'.format(corrupted_filepath))    \n",
    "else:\n",
    "    train2id = pd.read_csv(corrupted_filepath,\n",
    "        names=['head', 'tail', 'relation', 'label'], sep=' ', skiprows=0)\n",
    "    print('Corrupted file already exists: {}.'.format(corrupted_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read validation and test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'FB15k' :\n",
    "    valid2id = pd.read_csv(dataset_path + 'valid_neg.txt', sep='\\t', skiprows=0, names=['head', 'tail', 'relation', 'label'])\n",
    "    test2id = pd.read_csv(dataset_path + 'test_neg.txt', sep='\\t', skiprows=0, names=['head', 'tail', 'relation', 'label'])\n",
    "else:\n",
    "    valid2id_pos = pd.read_csv(dataset_path + 'valid2id.txt', sep=' ', skiprows=1, names=['head', 'tail', 'relation'])\n",
    "    valid2id_neg = pd.read_csv(dataset_path + 'valid2id_neg.txt', sep=' ', skiprows=1, names=['head', 'tail', 'relation'])\n",
    "    test2id_pos = pd.read_csv(dataset_path + 'test2id.txt', sep=' ', skiprows=1, names=['head', 'tail', 'relation'])\n",
    "    test2id_neg = pd.read_csv(dataset_path + 'test2id_neg.txt', sep=' ', skiprows=1, names=['head', 'tail', 'relation'])\n",
    "\n",
    "    valid2id_pos['label'] = 1\n",
    "    valid2id_neg['label'] = -1\n",
    "    test2id_pos['label'] = 1\n",
    "    test2id_neg['label'] = -1\n",
    "\n",
    "    valid2id = pd.concat((valid2id_pos, valid2id_neg))\n",
    "    test2id = pd.concat((test2id_pos, test2id_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train2id.head())\n",
    "display(valid2id.head())\n",
    "display(test2id.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info_df = pd.read_csv('{}/model_info.tsv'.format(import_path), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform model info into dict with only one \"row\"\n",
    "model_info = model_info_df.to_dict()\n",
    "for key,d in model_info.iteritems():\n",
    "    model_info[key] = d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = config.Config()\n",
    "dataset_path = \"./benchmarks/{}/\".format(model_info['dataset_name'])\n",
    "con.set_in_path(dataset_path)\n",
    "con.set_test_link_prediction(False)\n",
    "con.set_test_triple_classification(True)\n",
    "con.set_work_threads(multiprocessing.cpu_count())\n",
    "con.set_dimension(int(model_info['k']))\n",
    "con.score_norm = model_info['score_norm']\n",
    "con.init()\n",
    "con.set_model(embedding_model)\n",
    "con.import_variables(\"{}tf_model/model.vec.tf\".format(import_path)) # loading model via tensor library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Update Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in [train2id, valid2id, test2id]:\n",
    "    fold['label'] = con.classify(fold['head'], fold['tail'], fold['relation'])\n",
    "    fold['label'] = fold['label'].map(lambda x: 1 if x==1 else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode from id to names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_hat_df = pd.read_csv(g_hat_path_ids, names=['head', 'relation', 'tail'], sep='\\t', skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2id, id2entity = dataset_tools.read_name2id_file(dataset_path + 'entity2id.txt')\n",
    "relation2id, id2relation = dataset_tools.read_name2id_file(dataset_path + 'relation2id.txt')\n",
    "\n",
    "for fold in [train2id, valid2id, test2id, g_hat_df]:\n",
    "    fold['head'] = fold['head'].map(id2entity)\n",
    "    fold['tail'] = fold['tail'].map(id2entity)\n",
    "    fold['relation'] = fold['relation'].map(id2relation)\n",
    "\n",
    "# WARNING: at this stage we have transformed the dataframes,\n",
    "#   and entities and relations are not represented by ids anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save G^ decoded to names file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_hat_df.to_csv(g_hat_path_names, header=False, index=False, sep='\\t', columns=['head', 'relation', 'tail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PRA Experiment Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_g_hat = \"\"\"\n",
    "{{\n",
    "    \"graph\": {{\n",
    "        \"name\": \"g_hat_{}nn\",\n",
    "        \"relation sets\": [\n",
    "            {{\n",
    "                \"is kb\": false,\n",
    "                \"relation file\": \"{}\"\n",
    "            }}\n",
    "        ]\n",
    "    }},\n",
    "    \"split\": \"{}\",\n",
    "    \"operation\": {{\n",
    "        \"type\": \"create matrices\",\n",
    "        \"features\": {{\n",
    "            \"type\": \"subgraphs\",\n",
    "            \"path finder\": {{\n",
    "                \"type\": \"BfsPathFinder\",\n",
    "                \"number of steps\": 2\n",
    "            }},\n",
    "            \"feature extractors\": [\n",
    "                \"PraFeatureExtractor\"\n",
    "            ],\n",
    "            \"feature size\": -1\n",
    "        }}\n",
    "    }},\n",
    "    \"output\": {{ \"output matrices\": true }}\n",
    "}}\n",
    "\n",
    "\"\"\".format(knn_k, g_hat_path_names, split_name)\n",
    "spec_g_hat_fpath = '{}/experiment_specs/{}.json'.format(pra_explain_path, spec_g_hat_name)\n",
    "with open(spec_g_hat_fpath, 'w') as f:\n",
    "    f.write(spec_g_hat)\n",
    "print \"Spec file written: {}\".format(spec_g_hat_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate split (inside `./results/`) with random negative examples (bernoulli or uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import pra_setup\n",
    "\n",
    "pra_setup.create_split({'train': train2id, 'valid': valid2id, 'test': test2id},\n",
    "                       splits_dirpath=import_path+'/pra_explain/splits',\n",
    "                       split_name=split_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PRA (extract features for split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$pra_explain_path_abs\" \"$spec_g_hat_name\"\n",
    "(cd /home/arthurcgusmao/Projects/xkbc/algorithms/pra/; sbt \"runMain edu.cmu.ml.rtw.pra.experiments.ExperimentRunner $1 $2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features (paths) extracted and saved into:\\n{}\".format(os.path.abspath(pra_explain_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm /home/arthurcgusmao/Projects/xkbc/algorithms/OpenKE/./results/FB13/TransE/1524490825//pra_explain//results/ -r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
