{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tools.feature_matrices import parse_feature_matrices\n",
    "from tools import dataset_tools\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './benchmarks/FB13/'\n",
    "corrupted_data_path = '/Users/Alvinho/Documents/benchmarks/FB13/corrupted/train2id_bern_2to1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2id, id2entity = dataset_tools.read_name2id_file(dataset_path + 'entity2id.txt')\n",
    "relation2id, id2relation = dataset_tools.read_name2id_file(dataset_path + 'relation2id.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_train = pd.read_csv(dataset_path + 'train2id.txt', sep=' ', skiprows=1, names=['e1', 'e2', 'rel'])\n",
    "true_valid = pd.read_csv(dataset_path + 'valid2id.txt', sep=' ', skiprows=1, names=['e1', 'e2', 'rel'])\n",
    "true_test = pd.read_csv(dataset_path + 'test2id.txt', sep=' ', skiprows=1, names=['e1', 'e2', 'rel'])\n",
    "\n",
    "valid_neg = pd.read_csv(dataset_path + 'valid2id_neg.txt', sep=' ', skiprows=1, names=['e1', 'e2', 'rel'])\n",
    "test_neg = pd.read_csv(dataset_path + 'test2id_neg.txt', sep=' ', skiprows=1, names=['e1', 'e2', 'rel'])\n",
    "\n",
    "data = pd.concat([true_train, true_valid, true_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = set()\n",
    "ents.update(data.e1.unique())\n",
    "ents.update(data.e2.unique())\n",
    "\n",
    "print('Entities: {}'.format(len(ents)))\n",
    "print('Relations: {}'.format(len(data.rel.unique())))\n",
    "\n",
    "print('\\nTrain triples: {}'.format(len(true_train)))\n",
    "print('Valid triples: {}'.format(len(true_valid)))\n",
    "print('Test triples: {}'.format(len(true_test)))\n",
    "\n",
    "print('\\nAll triples: {}').format(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_id2relation(x):\n",
    "    return id2relation[x]\n",
    "\n",
    "def apply_id2entity(x):\n",
    "    return id2entity[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add relations and entities names to dataset\n",
    "true_test['rel_name'] = true_test['rel'].apply(apply_id2relation)\n",
    "true_test['head'] = true_test['e1'].apply(apply_id2entity)\n",
    "true_test['tail'] = true_test['e2'].apply(apply_id2entity)\n",
    "# Training data\n",
    "true_train['rel_name'] = true_train['rel'].apply(apply_id2relation)\n",
    "true_train['head'] = true_train['e1'].apply(apply_id2entity)\n",
    "true_train['tail'] = true_train['e2'].apply(apply_id2entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the feature matrixes (tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_relation = 'profession'\n",
    "data_path = './extract_feat__neg_by_random/' + target_relation\n",
    "split_data_path = '/Users/Alvinho/Documents/1524490825/pra_explain/splits/bern_2to1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_fpath = data_path + \"/train.tsv\"\n",
    "validation_matrix_fpath = data_path + \"/validation.tsv\"\n",
    "test_matrix_fpath = data_path + \"/test.tsv\"\n",
    "train_data, test_data = parse_feature_matrices(train_matrix_fpath, test_matrix_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_true_train = true_train[true_train['rel_name']==target_relation].copy()\n",
    "rel_true_train['true_label'] = np.ones(rel_true_train.shape[0])\n",
    "train_data = train_data.merge(rel_true_train[['head', 'tail', 'true_label']], how='left', on=['head', 'tail'])\n",
    "train_data = train_data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_true_test = true_test[true_test['rel_name']==target_relation].copy()\n",
    "rel_true_test['true_label'] = np.ones(rel_true_test.shape[0])\n",
    "test_data = test_data.merge(rel_true_test[['head', 'tail', 'true_label']], how='left', on=['head', 'tail'])\n",
    "test_data = test_data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate x (features) and y (labels)\n",
    "train_y = train_data.pop('label')\n",
    "true_train_y = train_data.pop('true_label')\n",
    "train_x = train_data.drop(['head', 'tail'], axis=1)\n",
    "\n",
    "test_y = test_data.pop('label')\n",
    "true_test_y = test_data.pop('true_label')\n",
    "test_x = test_data.drop(['head', 'tail'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "rel_true_train = true_train[true_train['rel_name']==target_relation]\n",
    "# Corrupted data \n",
    "corrupted_data = pd.read_csv(corrupted_data_path, sep=' ', skiprows=1, names=['e1', 'e2', 'rel', 'label'])\n",
    "# Add relations and entities names to dataset\n",
    "corrupted_data['rel_name'] = corrupted_data['rel'].apply(apply_id2relation)\n",
    "corrupted_data['e1_name'] = corrupted_data['e1'].apply(apply_id2entity)\n",
    "corrupted_data['e2_name'] = corrupted_data['e2'].apply(apply_id2entity)\n",
    "rel_corrupted_data = corrupted_data[corrupted_data['rel_name']==target_relation]\n",
    "# After spliting the data into relations\n",
    "split = pd.read_csv(split_data_path + '/' + target_relation + '/' + 'train.tsv', sep='\\t', skiprows=0, header=None)\n",
    "# After applying pra\n",
    "pra_output = pd.read_csv(train_matrix_fpath, sep='\\t', skiprows=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original data: \", rel_true_train.shape)\n",
    "print(\"Corrupted: \", rel_corrupted_data.shape)\n",
    "print(\"Split: \", split.shape)\n",
    "print(\"Pra Output: \", pra_output.shape)\n",
    "print(\"After Parsing: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic net is a logistic regression model which combines L1 and L2 regularizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'l1_ratio': [.1, .5, .7, .9, .95, .99, 1], 'alpha': [0.01, 0.001, 0.0001]}\n",
    "]\n",
    "#w_l1 = 0.5\n",
    "#w_l2 = 0.05\n",
    "#l1_ratio = w_l1 / (w_l1 + w_l2)\n",
    "# alpha = w_l1 + w_l2\n",
    "#alpha = 0.0001\n",
    "\n",
    "model = SGDClassifier(loss=\"log\", penalty=\"elasticnet\",\n",
    "                      max_iter=100000, tol=1e-3, class_weight=\"balanced\")\n",
    "clf = GridSearchCV(model, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = clf.best_params_['alpha']\n",
    "l1_ratio = clf.best_params_['l1_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(test_x, true_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = model.coef_.reshape(-1,1) # normalize(abs(model.coef_), norm='l1', axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_relevant_variables = pd.DataFrame(coefficients, columns=['scores'])\n",
    "most_relevant_variables['path'] = train_x.columns\n",
    "np_train_x = train_x.apply(pd.to_numeric)\n",
    "occurences = np.sum(np_train_x.as_matrix(), axis=0)\n",
    "most_relevant_variables['occurences'] = occurences\n",
    "most_relevant_variables = most_relevant_variables.sort_values(by=\"scores\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_most_relevant = pd.concat([most_relevant_variables.iloc[0:15], most_relevant_variables.iloc[-15:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_most_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Influencers\n",
    "most_relevant_variables.iloc[-15:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_relations = most_relevant_variables[most_relevant_variables['scores'] != 0].shape[0]\n",
    "total_relations = most_relevant_variables.shape[0]\n",
    "total_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_coefficients = np.repeat(coefficients.T, train_x.shape[0], axis=0)\n",
    "train_x = train_x.apply(pd.to_numeric)\n",
    "explanations = train_x.mul(repeated_coefficients, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reasons(row):    \n",
    "    reasons = row[row != 0]\n",
    "    string = ''\n",
    "    for reason, relevance in reasons.iteritems():\n",
    "        string += str(reason) + \" \" + str(relevance) + \" / \"\n",
    "    string = string[:-3]\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reasons2(row):    \n",
    "    reasons = row[row != 0]\n",
    "    output = pd.Series()\n",
    "    counter = 1\n",
    "    for reason, relevance in reasons.iteritems():\n",
    "        output['reason' + str(counter)] = reason\n",
    "        output['relevance' + str(counter)] = relevance\n",
    "        counter = counter + 1\n",
    "        if counter == 10:\n",
    "            break\n",
    "    for i in range(counter, 10):\n",
    "        output['reason' + str(i)] = \"n/a\"\n",
    "        output['relevance' + str(i)] = \"n/a\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(coefficients, data_type):\n",
    "    if data_type == 'train':\n",
    "        x = train_x\n",
    "        y = train_y\n",
    "        data = train_data\n",
    "    else:\n",
    "        x = test_x\n",
    "        y = test_y\n",
    "        data = test_data\n",
    "    final_reasons = pd.DataFrame()\n",
    "    final_reasons['head'] = data['head']\n",
    "    final_reasons['tail'] = data['tail']\n",
    "    repeated_coefficients = np.repeat(coefficients.T, x.shape[0], axis=0)\n",
    "    weighted_x = x.apply(pd.to_numeric)\n",
    "    explanations = weighted_x.mul(repeated_coefficients, axis=1)\n",
    "    motives = explanations.apply(get_reasons2, axis=1)\n",
    "    final_reasons = pd.concat([final_reasons, motives], axis=1)\n",
    "    answers = model.predict_proba(x)[:, 1]\n",
    "    final_reasons['y_hat'] = answers\n",
    "    final_reasons['y'] = y\n",
    "    return final_reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_reasons = explain(coefficients, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('death_causes.txt', 'w+') as f:\n",
    "    f.write(test_final_reasons.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_reasons.to_csv(\"death_causes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_reasons[(test_final_reasons['reason1']=='-gender-_gender-children-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "886./(886+92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model with different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(test_y, model.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_y, model.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test_y, model.predict(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced vs imbalanced weights for classes\n",
    "\n",
    "Let's see the difference between models when one don't balance the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", alpha=alpha, l1_ratio=l1_ratio,\n",
    "                      max_iter=1000, tol=1e-3)\n",
    "model2.fit(train_x, train_y)\n",
    "f1_score(test_y, model2.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(test_y, model2.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_y, model2.predict(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these experiments we see that using the option `class_weight=\"balanced\"` favored recall over precision (which is consistent to [here](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if everything is ok with the feature (x) dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train_x.columns) == set(test_x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the labels (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem, the logistic regression is simply outputting the mean for all cases!!! Holly, man!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_l1 = 0.5\n",
    "w_l2 = 0.01\n",
    "alpha = w_l1 + w_l2\n",
    "l1_ratio = w_l1 / (w_l1 + w_l2)\n",
    "\n",
    "# model = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", alpha=alpha, l1_ratio=l1_ratio)\n",
    "model = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", l1_ratio=l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {\n",
    "    \"x1\": 1,\n",
    "    \"x2\": 1,\n",
    "    \"x3\": 1,\n",
    "}\n",
    "d2 = {\n",
    "    \"x1\": 1,\n",
    "    \"x2\": 1,\n",
    "    \"x3\": 1,\n",
    "}\n",
    "d3 = {\n",
    "    \"x1\": 0,\n",
    "    \"x2\": 0,\n",
    "    \"x3\": 0,\n",
    "}\n",
    "d4 = {\n",
    "    \"x1\": 1,\n",
    "    \"x2\": 1,\n",
    "    \"x3\": 0,\n",
    "}\n",
    "train_x = pd.DataFrame([d1, d2, d3, d4])\n",
    "train_y = pd.DataFrame([1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = {\n",
    "    \"x1\": 1,\n",
    "    \"x2\": 1,\n",
    "    \"x3\": 0,\n",
    "}\n",
    "d5 = {\n",
    "    \"x1\": 1,\n",
    "    \"x2\": 0,\n",
    "    \"x3\": 0,\n",
    "}\n",
    "test_x = pd.DataFrame([d4, d5])\n",
    "test_y = pd.DataFrame([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('/Users/Alvinho/pra/extract_feat__neg_by_random/institution/institution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
