{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "from tools import dataset_tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config, models\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from original graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main variables\n",
    "dataset_name               = \"FB13\"\n",
    "embedding_model            = models.TransE\n",
    "model_timestamp            = '1524490825'\n",
    "\n",
    "# GPU settings\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # should be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './benchmarks/{}/'.format(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create original graph for PRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.pra_setup import create_graph_input\n",
    "\n",
    "create_graph_input(\n",
    "    dataset_path,\n",
    "    labels=['valid.tsv', 'test.tsv'] # folds that have labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will automatically run `ExperimentRunner`, without asking for class selection, for the specific dataset.\n",
    "\n",
    "Make sure the benchmark (original) graph is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the cell below will return an error, but the files needed probably will be generated correcly. This is happening because the `.json` spec files don't have all atributes, this is something to fix later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$dataset_name\"\n",
    "(cd /home/arthurcgusmao/Projects/xkbc/algorithms/pra/; sbt \"runMain edu.cmu.ml.rtw.pra.experiments.ExperimentRunner /home/arthurcgusmao/Projects/xkbc/algorithms/OpenKE/benchmarks/pra/ $1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate split (inside `./results/`) with random negative examples (bernoulli or uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate/Read Negative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rate = 2 # negative to positive ratio\n",
    "bern = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = 'bern' if bern else 'unif'\n",
    "corrupted_filename = 'train2id_{}_{}to1.txt'.format(distribution, neg_rate)\n",
    "corrupted_dirpath = dataset_path + '/corrupted/'\n",
    "corrupted_filepath = corrupted_dirpath + corrupted_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(corrupted_filepath):\n",
    "    # create corrupted dirpath if not exist\n",
    "    if not os.path.exists(corrupted_dirpath):\n",
    "        os.makedirs(corrupted_dirpath)\n",
    "    # generate corrupted set and save to disk in `corrupted` folder\n",
    "    corrupted = dataset_tools.generate_corrupted_training_examples(dataset_path,\n",
    "            neg_proportion=neg_rate, bern=bern)\n",
    "    train2id = pd.DataFrame(corrupted)\n",
    "    train2id.to_csv(corrupted_filepath,\n",
    "        columns=['head', 'tail', 'relation', 'label'], index=False, header=False, sep=' ')\n",
    "    print('Created corrupted file: {}.'.format(corrupted_filepath))    \n",
    "else:\n",
    "    train2id = pd.read_csv(corrupted_filepath,\n",
    "        names=['head', 'tail', 'relation', 'label'], sep=' ', skiprows=0)\n",
    "    print('Corrupted file already exists: {}.'.format(corrupted_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read validation and test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'FB15k' :\n",
    "    valid2id = pd.read_csv(dataset_path + 'valid_neg.txt', sep='\\t', skiprows=0, names=['head', 'tail', 'relation', 'label'])\n",
    "    test2id = pd.read_csv(dataset_path + 'test_neg.txt', sep='\\t', skiprows=0, names=['head', 'tail', 'relation', 'label'])\n",
    "else:\n",
    "    valid2id_pos = pd.read_csv(dataset_path + 'valid2id.txt', sep=' ', skiprows=1, names=['head', 'tail', 'relation'])\n",
    "    valid2id_neg = pd.read_csv(dataset_path + 'valid2id_neg.txt', sep=' ', skiprows=1, names=['head', 'tail', 'relation'])\n",
    "    test2id_pos = pd.read_csv(dataset_path + 'test2id.txt', sep=' ', skiprows=1, names=['head', 'tail', 'relation'])\n",
    "    test2id_neg = pd.read_csv(dataset_path + 'test2id_neg.txt', sep=' ', skiprows=1, names=['head', 'tail', 'relation'])\n",
    "\n",
    "    valid2id_pos['label'] = 1\n",
    "    valid2id_neg['label'] = -1\n",
    "    test2id_pos['label'] = 1\n",
    "    test2id_neg['label'] = -1\n",
    "\n",
    "    valid2id = pd.concat((valid2id_pos, valid2id_neg))\n",
    "    test2id = pd.concat((test2id_pos, test2id_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train2id.head())\n",
    "display(valid2id.head())\n",
    "display(test2id.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_path = './results/{}/{}/{}/'.format(\n",
    "    dataset_name,\n",
    "    embedding_model.__name__,\n",
    "    model_timestamp\n",
    ")\n",
    "\n",
    "model_info_df = pd.read_csv('{}/model_info.tsv'.format(import_path), sep='\\t')\n",
    "model_info_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform model info into dict with only one \"row\"\n",
    "model_info = model_info_df.to_dict()\n",
    "for key,d in model_info.iteritems():\n",
    "    model_info[key] = d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = config.Config()\n",
    "dataset_path = \"./benchmarks/{}/\".format(model_info['dataset_name'])\n",
    "con.set_in_path(dataset_path)\n",
    "con.set_test_link_prediction(False)\n",
    "con.set_test_triple_classification(True)\n",
    "con.set_work_threads(multiprocessing.cpu_count())\n",
    "con.set_dimension(int(model_info['k']))\n",
    "con.score_norm = model_info['score_norm']\n",
    "con.init()\n",
    "con.set_model(embedding_model)\n",
    "con.import_variables(\"{}tf_model/model.vec.tf\".format(import_path)) # loading model via tensor library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Update Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in [train2id, valid2id, test2id]:\n",
    "    fold['label'] = con.classify(fold['head'], fold['tail'], fold['relation'])\n",
    "    fold['label'] = fold['label'].map(lambda x: 1 if x==1 else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode from id to names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2id, id2entity = dataset_tools.read_name2id_file(dataset_path + 'entity2id.txt')\n",
    "relation2id, id2relation = dataset_tools.read_name2id_file(dataset_path + 'relation2id.txt')\n",
    "\n",
    "for fold in [train2id, valid2id, test2id]:\n",
    "    fold['head'] = fold['head'].map(id2entity)\n",
    "    fold['tail'] = fold['tail'].map(id2entity)\n",
    "    fold['relation'] = fold['relation'].map(id2relation)\n",
    "\n",
    "# WARNING: at this stage we have transformed the dataframes,\n",
    "#   and entities and relations are not represented by ids anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move PRA template to results (model) dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pra_explain_path = import_path + '/pra_explain/'\n",
    "split_name = '{}_{}to1'.format(distribution, neg_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move template to `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$pra_explain_path\"\n",
    "cp ./tools/pra_explain_results_template/* $1/ -R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change variables in `experiment_specs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$dataset_name\" \"$pra_explain_path\" \"$split_name\"\n",
    "sed -i \"s/_DATASETNAME_/$1/g\" $2/experiment_specs/*\n",
    "sed -i \"s/_SPLITNAME_/$3/g\" $2/experiment_specs/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import pra_setup\n",
    "\n",
    "pra_setup.create_split({'train': train2id, 'valid': valid2id, 'test': test2id},\n",
    "                       splits_dirpath=import_path+'/pra_explain/splits',\n",
    "                       split_name=split_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Paths for split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$pra_explain_path\"\n",
    "(cd /home/arthurcgusmao/Projects/xkbc/algorithms/pra/; sbt \"runMain edu.cmu.ml.rtw.pra.experiments.ExperimentRunner /home/arthurcgusmao/Projects/xkbc/algorithms/OpenKE/$1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features (paths) extracted and saved into:\\n{}\".format(os.path.abspath(pra_explain_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm /home/arthurcgusmao/Projects/xkbc/algorithms/OpenKE/./results/FB13/TransE/1524490825//pra_explain//results/ -r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
